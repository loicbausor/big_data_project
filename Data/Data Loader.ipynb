{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proprietary-dubai",
   "metadata": {},
   "source": [
    "# Data loader\n",
    "\n",
    "As explained in the Readme this notebook is not meant to be run again (you can if you can wait one night to collect the data).\n",
    "If you want to run it knowing this fact, download the full data manually [here]() and extract it in the data folder. \n",
    "\n",
    "You can just read the Notebook to understand how we first treat the data. In a nutshell the idea is the following :\n",
    "\n",
    "1. Get the XML data files and convert it into spark dataframe.\n",
    "\n",
    "2. Sample the __Posts__ table in order to have only the questions of the users and take only 10% of them.\n",
    "\n",
    "3. Sample the __users__, __badges__ & __tags__ in function of the posts we have. We also create a table __country_mapping__ to map users locations to their countries.\n",
    "\n",
    "4. Finnaly save all this new tables in the Data/sample folder !\n",
    "\n",
    "We already imported those samples [here](https://drive.google.com/drive/folders/1ddsBX4I4hZ8pordSKf5cHRaVBnNVOcKk). The other notebooks (Analysis, ML etc) are given with a function (`stack_overflow_functions.DataLoader.download_data`) which download the data if needed. So, once again, you will have nothing to do with the data loading. :) \n",
    "\n",
    "## Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Spark for data treatment\n",
    "import pyspark\n",
    "import sparknlp\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Packages to geolocate users\n",
    "from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Global variables\n",
    "seed = 2020 # Ensure reporductibility\n",
    "sample_size = 0.1 # Sample proportion for the posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-blackjack",
   "metadata": {},
   "source": [
    "### Custom functions to handle XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worth-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line22csv(line, tags_list):\n",
    "    \"\"\"Maps XML lines to a CSV format\"\"\"\n",
    "    results = []\n",
    "    offset=0\n",
    "    for i in tags_list:\n",
    "        val=\"\"\n",
    "        patt=i + \"=\"\n",
    "        ind=line.find(patt,offset)\n",
    "        if(ind==-1):\n",
    "            results.append(None)\n",
    "            continue\n",
    "        ind+=(len(i)+2)\n",
    "        val+='\\\"'\n",
    "        while(line[ind]!='\\\"'):\n",
    "            val+=line[ind]\n",
    "            ind+=1\n",
    "        val+='\\\"'\n",
    "        results.append(val.strip('\"'))\n",
    "        offset=ind\n",
    "    return tuple(results)\n",
    "\n",
    "\n",
    "def schema(fields):\n",
    "    \"\"\"\n",
    "    Creates a structure of df according to fields\n",
    "    When you first collect data it is a good practice to keep\n",
    "    every field in a STRING format to avoid any cast problem.\n",
    "    \"\"\"\n",
    "    return StructType([StructField(str(field),\n",
    "                                   StringType(),\n",
    "                                   True) for field in fields])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-series",
   "metadata": {},
   "source": [
    "### Starts the Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hidden-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()\n",
    "conf = pyspark.SparkConf() \n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "sqlcontext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-beast",
   "metadata": {},
   "source": [
    "## 1) Loading the XMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grave-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FieldNames for Users\n",
    "tags_fields = ['Id', 'TagName', 'Count', 'ExcerptPostId', 'WikiPostId']\n",
    "\n",
    "post_fields = ['Id', 'PostTypeId', 'AcceptedAnswerId', 'CreationDate', 'Score',\n",
    "               \"ViewCount\", \"Body\", \"OwnerUserId\", \"LastEditorUserId\",\n",
    "               \"LastEditorDisplayName\",\n",
    "               \"LastEditDate\", \"LastActivityDat\", \"Title\", \"Tags\",\n",
    "               \"AnswerCount\", \"CommentCount\",\"FavoriteCount\",\n",
    "               \"CommunityOwnedDate\", \"ContentLicense\"\n",
    "              ]\n",
    "\n",
    "badge_fields = [\"Id\", \"UserId\", \"Name\", \"Date\", \"Class\", \"TagBased\"]\n",
    "\n",
    "user_fields = [ \"Id\", \"Reputation\", \"CreationDate\", \"DisplayName\", \"Location\",\"Views\",\n",
    "               \"LastAccessDate\", \"AboutMe\", \"UpVotes\", \"DownVotes\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "civic-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_file = \"Tags.xml\"\n",
    "post_file = \"Posts.xml\"\n",
    "badge_file = \"Badges.xml\"\n",
    "user_file = \"Users.xml\"\n",
    "\n",
    "\n",
    "\n",
    "raw = (sc.textFile(post_file, 4))\n",
    "posts = (raw.map(lambda x:line22csv(x, post_fields))\n",
    "             .toDF(schema(post_fields))\n",
    "             .where(F.col('PostTypeId') == '1') # Only questions\n",
    "             .where(F.col('Tags').isNotNull()) # With at least one tag\n",
    "             .sample(False, sample_size, seed) # Sampled at ratio of sample_size\n",
    "             \n",
    "            )\n",
    "\n",
    "raw = (sc.textFile(tags_file, 4))\n",
    "tags = raw.map(lambda x:line22csv(x, tags_fields)).toDF(schema(tags_fields))\n",
    "\n",
    "raw = (sc.textFile(badge_file, 4))\n",
    "badges = raw.map(lambda x:line22csv(x, badge_fields)).toDF(schema(badge_fields))\n",
    "\n",
    "raw = (sc.textFile(user_file, 4))\n",
    "users = raw.map(lambda x:line22csv(x, user_fields)).toDF(schema(user_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-listening",
   "metadata": {},
   "source": [
    "### Estimates loading time of each tables and count rows\n",
    "#### Badge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "superior-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.4 ms, sys: 19 ms, total: 49.5 ms\n",
      "Wall time: 5min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39178979"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "badges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-values",
   "metadata": {},
   "source": [
    "#### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indian-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.93 ms, sys: 270 Âµs, total: 4.2 ms\n",
      "Wall time: 793 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60537"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tags.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-lambda",
   "metadata": {},
   "source": [
    "#### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "noble-bullet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.7 ms, sys: 14.4 ms, total: 33 ms\n",
      "Wall time: 2min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14080583"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "users.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-campaign",
   "metadata": {},
   "source": [
    "#### Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metallic-juice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 461 ms, sys: 175 ms, total: 636 ms\n",
      "Wall time: 1h 21min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2091001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "posts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-carpet",
   "metadata": {},
   "source": [
    "## Sample the data\n",
    "### Users & Country\n",
    "#### Sample users according to what we have in posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grateful-least",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071922"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_user_id = (posts\n",
    "                    .select(\"OwnerUserId\")\n",
    "                    .distinct()\n",
    "                   )\n",
    "users = (users\n",
    "         .join(distinct_user_id, users.Id == distinct_user_id.OwnerUserId)\n",
    "        )\n",
    "distinct_user_id.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-dietary",
   "metadata": {},
   "source": [
    "#### Get  countries infos of our users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-russia",
   "metadata": {},
   "source": [
    "To get the country informations about the users, we need to extract those informations from the 'Location' field. \n",
    "In order to do so we need to do some API calls with some libraries (here we use the `geopy` package but others can be used).\n",
    "To avoid any timeout issues due to parralelisation we decided to create a pandas table with all the informations about the user location (the table is quite small), then convert it into a spark df and join them efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-andrews",
   "metadata": {},
   "source": [
    "##### Functions to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faced-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the functions to get the country infos\n",
    "def geolocate_country(col):\n",
    "    \"\"\"\n",
    "    Geolocates the country given a location:\n",
    "    args:\n",
    "        col(str): The location\n",
    "    returns\n",
    "        country(str): The country associated (if found)\n",
    "    \"\"\"\n",
    "    if col is None or col == \"None\":\n",
    "        return (None, None, None, None, None)\n",
    "    geolocator = Nominatim(user_agent = \"aa\")\n",
    "    \n",
    "    try:\n",
    "        # Geolocate the country\n",
    "        location = geolocator.geocode(col, timeout=None, language=\"en\")\n",
    "        lat, lon = location.latitude, location.longitude\n",
    "        country = location.raw['display_name'].split(',')[-1].strip()\n",
    "    except:\n",
    "        country, lat, lon = None, None, None\n",
    "    \n",
    "    # Get the country ISO and the continent ISO\n",
    "    try:\n",
    "        cn_a2_code =  country_name_to_country_alpha2(country)\n",
    "    except:\n",
    "        cn_a2_code = None\n",
    "    try:\n",
    "        cn_continent = country_alpha2_to_continent_code(cn_a2_code)\n",
    "    except:\n",
    "        cn_continent = None\n",
    "    \n",
    "    return (country, cn_a2_code, cn_continent, lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-going",
   "metadata": {},
   "source": [
    "##### Creation of the location table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bulgarian-regard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location_mapping = (users\n",
    "                    .select(\"Location\")\n",
    "                    .distinct()\n",
    "                    .where(F.col(\"Location\").isNotNull())\n",
    "                    .where(F.col(\"Location\") != 'None')\n",
    "                    .toPandas()\n",
    "                   )\n",
    "location_mapping[\"Infos\"] = (location_mapping\n",
    "                               .loc[:,\"Location\"]\n",
    "                               .apply(geolocate_country)\n",
    "                              )\n",
    "\n",
    "location_mapping[['Country','Coun_iso','Cont_iso','Lat','Lon']] = \\\n",
    "pd.DataFrame(location_mapping.Infos.tolist()\n",
    "             ,index= location_mapping.index)\n",
    "location_mapping.drop('Infos', axis=1, inplace=True)\n",
    "location_mapping = sqlcontext.createDataFrame(location_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-identity",
   "metadata": {},
   "source": [
    "### Badges\n",
    "#### Sample users according to what we have in posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "strange-recipient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19572945"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_users = (posts\n",
    "        .select(\"OwnerUserId\")\n",
    "        .distinct()\n",
    "       )\n",
    "badges = (badges\n",
    "          .join(distinct_user_id, dist_users.OwnerUserId == badges.UserId)\n",
    "         )\n",
    "badges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-cleveland",
   "metadata": {},
   "source": [
    "## Stores all the preparation into parquet format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "uniform-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_out_file = \"sample/Tags\"\n",
    "post_out_file = \"sample/Posts\"\n",
    "badge_out_file = \"sample/Badges\"\n",
    "user_out_file = \"sample/Users\"\n",
    "location_out_file = \"sample/Country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bacterial-pharmacy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts transformation for: Data/sample/Tags\n",
      "Ended succesfully\n",
      "Starts transformation for: Data/sample/Badges\n",
      "Ended succesfully\n",
      "Starts transformation for: Data/sample/Users\n",
      "Ended succesfully\n",
      "Starts transformation for: Data/sample/Posts\n",
      "Ended succesfully\n",
      "Starts transformation for: Data/sample/Country\n",
      "Ended succesfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 47678)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Folder path to save processed file\n",
    "lst_df = [\n",
    "    (tags, tags_out_file),\n",
    "    (badges, badge_out_file),\n",
    "    (users, user_out_file),\n",
    "    (posts, post_out_file),\n",
    "    (location_mapping, location_out_file)\n",
    "]\n",
    "\n",
    "for elt in lst_df:\n",
    "    print(\"Starts transformation for: \" + elt[1])\n",
    "    elt[0].write.parquet(elt[1])\n",
    "    print(\"Ended succesfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
