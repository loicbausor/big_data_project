{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "meaning-calculation",
   "metadata": {},
   "source": [
    "# ML : Multilabel classification\n",
    "\n",
    "For this last part we do some Multilabel classification of the tags based on the text of each questions.\n",
    "In order to do so we use two frameworks : Spark MLLIB & Elephas which allows to connect spark pipeline with Keras.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "closing-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ['HOME'])\n",
    "\n",
    "import stack_overflow_functions.DataLoader as data_loader\n",
    "import stack_overflow_functions.DataTransformation as data_transfo\n",
    "from pycountry_convert import country_name_to_country_alpha3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import patoolib\n",
    "import gdown\n",
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql.types import StructField, StructType, StringType, ArrayType,IntegerType\n",
    "\n",
    "import pyspark\n",
    "import sparknlp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n",
    "from geopy.geocoders import Nominatim\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "import pandas as pd\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import geopandas as gpd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "differential-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()\n",
    "conf = (pyspark\n",
    "        .SparkConf()\n",
    "        .set(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "       )\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "sqlcontext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.2 ms, sys: 4.74 ms, total: 32 ms\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "post_dir = \"Data/sample/Posts\"\n",
    "posts = (sqlcontext\n",
    "         .read\n",
    "         .format(\"parquet\")\n",
    "         .option(\"header\",True)\n",
    "         .load(post_dir)\n",
    "         .sample(False, 0.01)\n",
    "         .select(\"Id\",\n",
    "                  F.concat_ws(' ',F.col('Title'),F.col('Body')).alias(\"full_text\"),\n",
    "                  \"Tags\"\n",
    "                )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exotic-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_split =tags_split = F.regexp_replace( F.regexp_replace(\n",
    "    F.regexp_replace(F.col('Tags'), '&lt;', ''), \"&gt;\", \"<split_token>\"), \" \", \"\")\n",
    "\n",
    "udf_drop = F.udf(lambda x: re.sub(\"'\",\"\",str(x[:-1])[1:-1]) if isinstance(x,list) else None,StringType())\n",
    "\n",
    "posts = (\n",
    "    posts\n",
    "    .withColumn('Splitted_tags', tags_split)\n",
    "    .withColumn('Splitted_tags', F.split(F.col(\"Splitted_tags\"), \"<split_token>\"))\n",
    "    .withColumn('Splitted_tags', udf_drop(F.col(\"Splitted_tags\")))\n",
    "    .withColumn('Splitted_tags', F.split(F.col(\"Splitted_tags\"),\",\"))\n",
    "    .drop('Tags')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "august-calgary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "input_col = \"full_text\"\n",
    "clean_up_patterns = [\n",
    "                    \"p&gt;\"\n",
    "                    ,\"&.*?;\\space\"\n",
    "                    ,'&.*?;'                \n",
    "                    ,\"/.*?;\"\n",
    "                    ,\"/code\"\n",
    "                    ,\"/pre\"\n",
    "                    ,'/p'\n",
    "                    ,\"/a\"\n",
    "                    ,\"href=\"\n",
    "                    ,\"lt;\"\n",
    "                    ,\"gt;\"\n",
    "                    ,\"[^\\w\\s]\"\n",
    "                    ,r\"\\b\\d+\\b\"\n",
    "                  ]\n",
    "\n",
    "\n",
    "# Document assembler : Tokenize our text\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(input_col) \\\n",
    "    .setOutputCol('_intermediate_results')\n",
    "\n",
    "# Document normalizer : Normalize the document\n",
    "# by lowercasing, removing non utf8 chars\n",
    "# and remove regex oattern defined\n",
    "doc_norm = DocumentNormalizer() \\\n",
    "    .setInputCols(\"_intermediate_results\") \\\n",
    "    .setOutputCol(input_col + \"_cleaned\") \\\n",
    "    .setAction(\"clean\") \\\n",
    "    .setPatterns(clean_up_patterns) \\\n",
    "    .setReplacement(\" \") \\\n",
    "    .setPolicy(\"pretty_all\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# Document tokenizer : allows to remove\n",
    "# undesired tokens (punctuations etc.)\n",
    "# prepare the colums for the stopwords \n",
    "# remover\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([input_col + \"_cleaned\"]) \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setSplitChars(['-']) \\\n",
    "    .setContextChars(['(', ')', '?', '!']) \\\n",
    "    .setSplitPattern(\"'\") \\\n",
    "    .setMaxLength(0) \\\n",
    "    .setMaxLength(99999) \\\n",
    "    .setCaseSensitiveExceptions(False)\n",
    "\n",
    "\n",
    "# StopWordsCleaner : remove \n",
    "# the stopwords based on\n",
    "# a predifined list\n",
    "Stop_words_cleaner = (\n",
    "    StopWordsCleaner()\n",
    "    .pretrained(\"stopwords_en\", \"en\")\n",
    "    .setInputCols([\"token\"])\n",
    "    .setOutputCol(input_col + \"_without_stopwords\") \n",
    "    .setCaseSensitive(False) \n",
    "    .setLazyAnnotator(False)\n",
    ")\n",
    "\n",
    "# Lemmatize the text \n",
    "# thanks to the lemmatizing tab\n",
    "# defined above\n",
    "Lemmatizer_cleaner = (\n",
    "    Lemmatizer() \n",
    "    .setInputCols([input_col + \"_without_stopwords\"]) \n",
    "    .setOutputCol(input_col + \"_lemmatized\") \n",
    "    .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\") \n",
    "    .setLazyAnnotator(False)\n",
    ")\n",
    "\n",
    "\n",
    "# Creates thepipeline\n",
    "cleaning_pipeline = (\n",
    "    Pipeline() \n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        doc_norm,\n",
    "        tokenizer,\n",
    "        #Document_cleaner,\n",
    "        Stop_words_cleaner,\n",
    "        Lemmatizer_cleaner])\n",
    ")\n",
    "\n",
    "\n",
    "posts_ml = (\n",
    "    cleaning_pipeline\n",
    "    .fit(posts)\n",
    "    .transform(posts)\n",
    "    .select(F.col(\"Id\"),\n",
    "            F.col(input_col),\n",
    "            F.col(input_col + \"_lemmatized.result\"),\n",
    "            F.col(\"Splitted_tags\")\n",
    "           )\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "documentary-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "all_tags = (\n",
    "            posts_ml\n",
    "                .select(F.explode(\"Splitted_tags\").alias('t'))\n",
    "                .select(F.trim(\"t\").alias('t'))\n",
    "                .groupby(\"t\")\n",
    "                .count()\n",
    "                .filter(F.col('count') >= 10)\n",
    "                .collect()\n",
    ")\n",
    "i=0\n",
    "match_tags = {}\n",
    "for tag in all_tags:\n",
    "    m_tag = tag['t'].strip()\n",
    "    match_tags[m_tag] = i\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "def map_tags(tag_list):\n",
    "    label = np.zeros(len(match_tags.keys()),dtype=int)\n",
    "    mask = []\n",
    "    for tag in tag_list:\n",
    "        tag_m = tag.strip()\n",
    "        for key, value in match_tags.items():\n",
    "            if tag_m == key:\n",
    "                mask.append(value)\n",
    "                pass\n",
    "    label[mask] = 1\n",
    "    return label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "active-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_map = F.udf(map_tags, StringType())\n",
    "posts_ml = posts_ml.withColumn(\"label_to_encode\", udf_map('Splitted_tags'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spare-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_ml=posts_ml.withColumn(\"lemma_text\",F.concat_ws(\" \", \"result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amazing-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "# TF\n",
    "cv = CountVectorizer(inputCol=\"result\", outputCol=\"tf_features\", vocabSize=100, minDF=0)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
    "\n",
    "# Label encoder \n",
    "label_string= StringIndexer(inputCol=\"label_to_encode\", outputCol =\"label\")\n",
    "\n",
    "# Logistic regression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001,family=\"multinomial\")\n",
    "pipeline = Pipeline(stages=[cv, idf, label_string, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(posts_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "understood-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------+--------------------+--------------------+----------+\n",
      "|      Id|           full_text|              result|       Splitted_tags|     label_to_encode|          lemma_text|         tf_features|            features| label|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------+--------------------+--------------------+----------+\n",
      "|63215921|Changing Google C...|[change, google, ...|[python-3.x,  goo...|[0, 0, 0, 0, 0, 0...|change google clo...|(100,[14,30,33,41...|(100,[14,30,33,41...|4003.0|[6.82200264777294...|[0.06237101207997...|       0.0|\n",
      "|63219559|Configuring Ident...|[configure, ident...|[c#,  asp.net-cor...|[0, 0, 0, 0, 0, 0...|configure identit...|(100,[0,1,2,3,4,7...|(100,[0,1,2,3,4,7...| 221.0|[3.86620816424095...|[1.46452563062122...|     221.0|\n",
      "|63221301|ManyToOne MyBatis...|[manytoone, mybat...|[java,  sql,  myb...|[0, 0, 0, 0, 0, 0...|manytoone mybatis...|(100,[0,1,2,3,4,7...|(100,[0,1,2,3,4,7...| 539.0|[3.65418578100098...|[0.00217777135674...|      55.0|\n",
      "|63223117|Errors after addi...|[error, add, type...|[typescript,  rea...|[0, 0, 0, 0, 0, 0...|error add typescr...|(100,[0,1,10,14,1...|(100,[0,1,10,14,1...|3179.0|[7.01678409198417...|[0.07492520602798...|       0.0|\n",
      "|63223298|Issue with backgr...|[issue, backgroun...|[html,  twitter-b...|[0, 0, 0, 0, 0, 0...|issue background ...|(100,[0,1,2,3,14,...|(100,[0,1,2,3,14,...|5340.0|[5.13606857070265...|[0.01133414357437...|      10.0|\n",
      "|63225130|Data cleaning whi...|[data, clean, web...|[python,  pandas,...|[0, 0, 0, 0, 0, 0...|data clean web sc...|(100,[0,1,2,3,5,1...|(100,[0,1,2,3,5,1...|4753.0|[4.86492657648364...|[0.00998246227414...|       0.0|\n",
      "|63225332|Using nested setT...|[nest, settimeout...|[javascript,  ani...|[0, 0, 0, 0, 0, 0...|nest settimeout c...|(100,[0,1,2,8,15,...|(100,[0,1,2,8,15,...| 860.0|[0.22446419891543...|[2.40181007021285...|     116.0|\n",
      "|63229390|How to create tre...|[create, tree, vi...|[javascript,  ang...|[0, 0, 0, 0, 0, 0...|create tree view ...|(100,[0,1,2,4,16,...|(100,[0,1,2,4,16,...|7849.0|[8.14740656116767...|[0.21515989407727...|       0.0|\n",
      "|63231620|How to make the s...|[make, scroll, bu...|[javascript,  htm...|[0, 0, 0, 0, 0, 0...|make scroll butto...|(100,[0,1,3,4,5,1...|(100,[0,1,3,4,5,1...|  38.0|[5.56505548881216...|[0.01965695528858...|      10.0|\n",
      "|63231877|Garbled character...|[garbled, charact...|[python,  csv,  g...|[0, 0, 0, 0, 0, 0...|garbled character...|(100,[0,2,6,14,27...|(100,[0,2,6,14,27...| 204.0|[5.09304152992364...|[0.00975871057878...|       3.0|\n",
      "|60494444|Using ElasticSear...|[elasticsearch, m...|[elasticsearch,  ...|[0, 0, 0, 0, 0, 0...|elasticsearch map...|(100,[0,1,4,10,16...|(100,[0,1,4,10,16...|  57.0|[10.2324736539866...|[0.55517322960373...|       0.0|\n",
      "|60495207|jmeter - How can ...|[jmeter, store, d...|     [java,  jmeter]|[0, 0, 0, 0, 0, 0...|jmeter store data...|(100,[0,1,3,4,5,1...|(100,[0,1,3,4,5,1...|1097.0|[6.80997213900962...|[0.05953445725496...|       0.0|\n",
      "|60498983|Redirect old doma...|[redirect, domain...|         [.htaccess]|[0, 0, 0, 0, 0, 0...|redirect domain d...|(100,[0,1,14,15,2...|(100,[0,1,14,15,2...| 234.0|[7.15355344448892...|[0.06397707238812...|       0.0|\n",
      "|60503464|How to properly w...|[properly, write,...|[docker,  dockerf...|[0, 0, 0, 0, 0, 0...|properly write do...|(100,[0,1,27,30,3...|(100,[0,1,27,30,3...|  88.0|[5.31437486465112...|[0.01411225538599...|      26.0|\n",
      "|61625906|How to get a noti...|[notification, go...|[windows,  google...|[0, 0, 0, 0, 0, 0...|notification goog...|(100,[14,27,33,41...|(100,[14,27,33,41...|4436.0|[6.88282732298852...|[0.07185578644647...|       0.0|\n",
      "|61629785|PageView vs Routi...|[pageview, rout, ...|[flutter,  routes...|[0, 0, 0, 0, 0, 0...|pageview rout flu...|(100,[34,37,60],[...|(100,[34,37,60],[...|  76.0|[7.09283013495738...|[0.08451622724886...|       0.0|\n",
      "|61631157|Vim - visually se...|[vim, visually, s...|[vim,  textselect...|[0, 0, 0, 0, 0, 0...|vim visually sele...|(100,[19,24,29,44...|(100,[19,24,29,44...| 102.0|[6.49114693937496...|[0.04934425552493...|       0.0|\n",
      "|61632131|speech recognitio...|[speech, recognit...|[python,  python-...|[0, 0, 0, 0, 0, 0...|speech recognitio...|(100,[0,1,10,15,1...|(100,[0,1,10,15,1...|  27.0|[5.23260471476492...|[0.01306337554494...|       3.0|\n",
      "|61632236|How to build a Li...|[build, libra, te...|             [libra]|[0, 0, 0, 0, 0, 0...|build libra testn...|    (100,[46],[2.0])|(100,[46],[4.6397...|   0.0|[7.42751453778764...|[0.11159858040982...|       0.0|\n",
      "|61635077|Hosting internal ...|[host, internal, ...|[amazon-web-servi...|[0, 0, 0, 0, 0, 0...|host internal s3 ...|(100,[14,15,21,25...|(100,[14,15,21,25...| 325.0|[6.77107281635570...|[0.06416011836077...|       0.0|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(posts_ml).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-young",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-probe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-rogers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-cardiff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
